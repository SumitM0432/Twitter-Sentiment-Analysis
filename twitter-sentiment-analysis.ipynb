{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset of tweets\n",
    "\n",
    "df_train=pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 3 columns):\n",
      "id       31962 non-null int64\n",
      "label    31962 non-null int64\n",
      "tweet    31962 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 749.2+ KB\n",
      "None\n",
      "   id                        ...                                                                      tweet\n",
      "0   1                        ...                           @user when a father is dysfunctional and is s...\n",
      "1   2                        ...                          @user @user thanks for #lyft credit i can't us...\n",
      "2   3                        ...                                                        bihday your majesty\n",
      "3   4                        ...                          #model   i love u take with u all the time in ...\n",
      "4   5                        ...                                     factsguide: society now    #motivation\n",
      "\n",
      "[5 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Information about the dataset\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Class Distribution\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         @user when a father is dysfunctional and is s...\n",
       "1        @user @user thanks for #lyft credit i can't us...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "5        [2/2] huge fan fare and big talking before the...\n",
       "6         @user camping tomorrow @user @user @user @use...\n",
       "7        the next school year is the year for exams.ð...\n",
       "8        we won!!! love the land!!! #allin #cavs #champ...\n",
       "9         @user @user welcome here !  i'm   it's so #gr...\n",
       "10        â #ireland consumer price index (mom) climb...\n",
       "11       we are so selfish. #orlando #standwithorlando ...\n",
       "12       i get to see my daddy today!!   #80days #getti...\n",
       "13       @user #cnn calls #michigan middle school 'buil...\n",
       "14       no comment!  in #australia   #opkillingbay #se...\n",
       "15       ouch...junior is angryð#got7 #junior #yugyo...\n",
       "16       i am thankful for having a paner. #thankful #p...\n",
       "17                                  retweet if you agree! \n",
       "18       its #friday! ð smiles all around via ig use...\n",
       "19       as we all know, essential oils are not made of...\n",
       "20       #euro2016 people blaming ha for conceded goal ...\n",
       "21       sad little dude..   #badday #coneofshame #cats...\n",
       "22       product of the day: happy man #wine tool  who'...\n",
       "23         @user @user lumpy says i am a . prove it lumpy.\n",
       "24        @user #tgif   #ff to my #gamedev #indiedev #i...\n",
       "25       beautiful sign by vendor 80 for $45.00!! #upsi...\n",
       "26        @user all #smiles when #media is   !! ðð...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28             happy father's day @user ðððð  \n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "31932                                 @user thanks gemma  \n",
       "31933    @user judd is a  &amp; #homophobic #freemilo #...\n",
       "31934    lady banned from kentucky mall. @user  #jcpenn...\n",
       "31935    ugh i'm trying to enjoy my happy hour drink &a...\n",
       "31936    want to know how to live a   life? do more thi...\n",
       "31937                                   love island ð  \n",
       "31938    my fav actor #vijaysethupathi ! my fav actress...\n",
       "31939        whew  ð\n",
       " it's a productive and   #friday!!!\n",
       "31940                   @user she's finally here! @user   \n",
       "31941    passed first year of uni #yay #love #pass #uni...\n",
       "31942    this week is flying by   #humpday - #wednesday...\n",
       "31943     @user modeling photoshoot this friday yay #mo...\n",
       "31944    you're surrounded by people who love you (even...\n",
       "31945    feel like... ðð¶ð #dog #summer #hot #h...\n",
       "31946    @user omfg i'm offended! i'm a  mailbox and i'...\n",
       "31947    @user @user you don't have the balls to hashta...\n",
       "31948     makes you ask yourself, who am i? then am i a...\n",
       "31949    hear one of my new songs! don't go - katie ell...\n",
       "31950     @user you can try to 'tail' us to stop, 'butt...\n",
       "31951    i've just posted a new blog: #secondlife #lone...\n",
       "31952                  @user you went too far with @user  \n",
       "31953    good morning #instagram #shower #water #berlin...\n",
       "31954    #holiday   bull up: you will dominate your bul...\n",
       "31955    less than 2 weeks ð\n",
       "ðð¼ð¹ððµ @us...\n",
       "31956    off fishing tomorrow @user carnt wait first ti...\n",
       "31957    ate @user isz that youuu?ðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31960    @user #sikh #temple vandalised in in #calgary,...\n",
       "31961                     thank you @user for you follow  \n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing the tweets and the labels\n",
    "\n",
    "tweets = df_train['tweet'].str.lower()\n",
    "Y = df_train['label']\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         user when a father is dysfunctional and is so...\n",
       "1        user user thanks for hash credit i can't use c...\n",
       "2                                      bihday your majesty\n",
       "3        hash   i love u take with u all the time in ur...\n",
       "4                          factsguide: society now    hash\n",
       "5        [2/2] huge fan fare and big talking before the...\n",
       "6         user camping tomorrow user user user user use...\n",
       "7        the next school year is the year for exams.ð...\n",
       "8        we won!!! love the land!!! hash hash hash hash...\n",
       "9          user user welcome here !  i'm   it's so hash ! \n",
       "10        â hash consumer price index (mom) climbed f...\n",
       "11       we are so selfish. hash hash hash hash hash ha...\n",
       "12               i get to see my daddy today!!   hash hash\n",
       "13       user hash calls hash middle school 'build the ...\n",
       "14        no comment!  in hash   hash hash hash hash  hash\n",
       "15        ouch...junior is angryðhash hash hash   hash \n",
       "16        i am thankful for having a paner. hash hash     \n",
       "17                                  retweet if you agree! \n",
       "18       its hash ð smiles all around via ig user: u...\n",
       "19       as we all know, essential oils are not made of...\n",
       "20       hash people blaming ha for conceded goal was i...\n",
       "21       sad little dude..   hash hash hash hash hash h...\n",
       "22       product of the day: happy man hash tool  who's...\n",
       "23           user user lumpy says i am a . prove it lumpy.\n",
       "24        user hash   hash to my hash hash hash hash us...\n",
       "25       beautiful sign by vendor 80 for $45.00!! hash ...\n",
       "26        user all hash when hash is   !! ðð hash...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28              happy father's day user ðððð  \n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "31932                                  user thanks gemma  \n",
       "31933    user judd is a  &amp; hash hash hash hash hash...\n",
       "31934    lady banned from kentucky mall. user  hash hash  \n",
       "31935    ugh i'm trying to enjoy my happy hour drink &a...\n",
       "31936    want to know how to live a   life? do more thi...\n",
       "31937                                   love island ð  \n",
       "31938    my fav actor hash ! my fav actress user ! my m...\n",
       "31939              whew  ð\n",
       " it's a productive and   hash\n",
       "31940                     user she's finally here! user   \n",
       "31941    passed first year of uni hash hash hash hash h...\n",
       "31942      this week is flying by   hash - hash hash hash \n",
       "31943     user modeling photoshoot this friday yay hash...\n",
       "31944    you're surrounded by people who love you (even...\n",
       "31945    feel like... ðð¶ð hash hash hash hash ...\n",
       "31946    user omfg i'm offended! i'm a  mailbox and i'm...\n",
       "31947    user user you don't have the balls to hashtag ...\n",
       "31948     makes you ask yourself, who am i? then am i a...\n",
       "31949    hear one of my new songs! don't go - katie ell...\n",
       "31950     user you can try to 'tail' us to stop, 'butt'...\n",
       "31951       i've just posted a new blog: hash hash hash   \n",
       "31952                    user you went too far with user  \n",
       "31953    good morning hash hash hash hash hash   hash h...\n",
       "31954    hash   bull up: you will dominate your bull an...\n",
       "31955    less than 2 weeks ð\n",
       "ðð¼ð¹ððµ use...\n",
       "31956    off fishing tomorrow user carnt wait first tim...\n",
       "31957    ate user isz that youuu?ðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31960    user hash hash vandalised in in hash hash cond...\n",
       "31961                      thank you user for you follow  \n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing @handle with the word USER\n",
    "\n",
    "tweets = tweets.str.replace(r'@[\\S]+', 'user')\n",
    "\n",
    "# Replacing the Hast tag with the word hASH\n",
    "\n",
    "tweets = tweets.str.replace(r'#(\\S+)','hash')\n",
    "\n",
    "# Removing the all the Retweets\n",
    "\n",
    "tweets = tweets.str.replace(r'\\brt\\b',' ')\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <th>0</th>\n",
       "      <td>www.flybcc.com</td>\n",
       "      <td>www.flybcc.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6375</th>\n",
       "      <th>0</th>\n",
       "      <td>www...</td>\n",
       "      <td>www...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8084</th>\n",
       "      <th>0</th>\n",
       "      <td>www.drunk</td>\n",
       "      <td>www.drunk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8484</th>\n",
       "      <th>0</th>\n",
       "      <td>www.smokeweedeatbacon.com</td>\n",
       "      <td>www.smokeweedeatbacon.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <th>0</th>\n",
       "      <td>www.alvarum/heloiseetlespremas</td>\n",
       "      <td>www.alvarum/heloiseetlespremas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25745</th>\n",
       "      <th>0</th>\n",
       "      <td>www...</td>\n",
       "      <td>www...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0 ...   2\n",
       "      match                                 ...    \n",
       "1111  0                      www.flybcc.com ... NaN\n",
       "6375  0                              www... ... NaN\n",
       "8084  0                           www.drunk ... NaN\n",
       "8484  0           www.smokeweedeatbacon.com ... NaN\n",
       "8660  0      www.alvarum/heloiseetlespremas ... NaN\n",
       "25745 0                              www... ... NaN\n",
       "\n",
       "[6 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['tweet'].str.extractall(r'((www\\.[\\S]+)|(http?://[\\S]+))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the URL or Web Address\n",
    "\n",
    "tweets = tweets.str.replace(r'((www\\.[\\S]+)|(http?://[\\S]+))','URL')\n",
    "\n",
    "# Replacing Two or more dots with one\n",
    "\n",
    "tweets = tweets.str.replace(r'\\.{2,}', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        user when a father is dysfunctional and is so ...\n",
       "1        user user thanks for hash credit i can t use c...\n",
       "2                                      bihday your majesty\n",
       "3             hash i love u take with u all the time in ur\n",
       "4                              factsguide society now hash\n",
       "5        2 2 huge fan fare and big talking before they ...\n",
       "6        user camping tomorrow user user user user user...\n",
       "7        the next school year is the year for exams can...\n",
       "8            we won love the land hash hash hash hash hash\n",
       "9                  user user welcome here i m it s so hash\n",
       "10       hash consumer price index mom climbed from pre...\n",
       "11       we are so selfish hash hash hash hash hash has...\n",
       "12                   i get to see my daddy today hash hash\n",
       "13       user hash calls hash middle school build the w...\n",
       "14             no comment in hash hash hash hash hash hash\n",
       "15                ouch junior is angry hash hash hash hash\n",
       "16              i am thankful for having a paner hash hash\n",
       "17                                    retweet if you agree\n",
       "18       its hash smiles all around via ig user user ha...\n",
       "19       as we all know essential oils are not made of ...\n",
       "20       hash people blaming ha for conceded goal was i...\n",
       "21           sad little dude hash hash hash hash hash hash\n",
       "22       product of the day happy man hash tool who s i...\n",
       "23              user user lumpy says i am a prove it lumpy\n",
       "24       user hash hash to my hash hash hash hash user ...\n",
       "25       beautiful sign by vendor 80 for 45 00 hash has...\n",
       "26       user all hash when hash is hash in hash hash s...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28                                 happy father s day user\n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "31932                                    user thanks gemma\n",
       "31933    user judd is a amp hash hash hash hash hash ha...\n",
       "31934        lady banned from kentucky mall user hash hash\n",
       "31935    ugh i m trying to enjoy my happy hour drink am...\n",
       "31936    want to know how to live a life do more things...\n",
       "31937                                          love island\n",
       "31938    my fav actor hash my fav actress user my most ...\n",
       "31939                      whew it s a productive and hash\n",
       "31940                         user she s finally here user\n",
       "31941    passed first year of uni hash hash hash hash h...\n",
       "31942           this week is flying by hash hash hash hash\n",
       "31943    user modeling photoshoot this friday yay hash ...\n",
       "31944    you re surrounded by people who love you even ...\n",
       "31945         feel like hash hash hash hash hash hash hash\n",
       "31946    user omfg i m offended i m a mailbox and i m p...\n",
       "31947    user user you don t have the balls to hashtag ...\n",
       "31948    makes you ask yourself who am i then am i anyb...\n",
       "31949    hear one of my new songs don t go katie ellie ...\n",
       "31950    user you can try to tail us to stop butt we re...\n",
       "31951           i ve just posted a new blog hash hash hash\n",
       "31952                      user you went too far with user\n",
       "31953    good morning hash hash hash hash hash hash has...\n",
       "31954    hash bull up you will dominate your bull and y...\n",
       "31955                          less than 2 weeks user hash\n",
       "31956    off fishing tomorrow user carnt wait first tim...\n",
       "31957                              ate user isz that youuu\n",
       "31958    to see nina turner on the airwaves trying to w...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31960    user hash hash vandalised in in hash hash cond...\n",
       "31961                        thank you user for you follow\n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing all the special Characters\n",
    "\n",
    "tweets = tweets.str.replace(r'[^\\w\\d\\s]',' ')\n",
    "\n",
    "# Removing all the non ASCII characters\n",
    "\n",
    "tweets = tweets.str.replace(r'[^\\x00-\\x7F]+',' ')\n",
    "\n",
    "# Removing the leading and trailing Whitespaces\n",
    "\n",
    "tweets = tweets.str.replace(r'^\\s+|\\s+?$','')\n",
    "\n",
    "# Replacing multiple Spaces with Single Space\n",
    "\n",
    "tweets = tweets.str.replace(r'\\s+',' ')\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        user father dysfunct selfish drag kid dysfunct...\n",
       "1        user user thank hash credit use caus offer whe...\n",
       "2                                           bihday majesti\n",
       "3                               hash love u take u time ur\n",
       "4                                   factsguid societi hash\n",
       "5        2 2 huge fan fare big talk leav chao pay dispu...\n",
       "6        user camp tomorrow user user user user user us...\n",
       "7        next school year year exam think hash hash has...\n",
       "8                       love land hash hash hash hash hash\n",
       "9                                    user user welcom hash\n",
       "10       hash consum price index mom climb previous 0 2...\n",
       "11       selfish hash hash hash hash hash hash hash has...\n",
       "12                           get see daddi today hash hash\n",
       "13       user hash call hash middl school build wall ch...\n",
       "14                   comment hash hash hash hash hash hash\n",
       "15                   ouch junior angri hash hash hash hash\n",
       "16                                   thank paner hash hash\n",
       "17                                            retweet agre\n",
       "18       hash smile around via ig user user hash make p...\n",
       "19                            know essenti oil made chemic\n",
       "20       hash peopl blame ha conced goal fat rooney gav...\n",
       "21            sad littl dude hash hash hash hash hash hash\n",
       "22       product day happi man hash tool hash time open...\n",
       "23                         user user lumpi say prove lumpi\n",
       "24       user hash hash hash hash hash hash user user u...\n",
       "25              beauti sign vendor 80 45 00 hash hash hash\n",
       "26          user hash hash hash hash hash sunday hash love\n",
       "27                   great panel mediat public servic hash\n",
       "28                                   happi father day user\n",
       "29       50 peopl went nightclub good night 1 man actio...\n",
       "                               ...                        \n",
       "31932                                     user thank gemma\n",
       "31933    user judd amp hash hash hash hash hash hash ha...\n",
       "31934                ladi ban kentucki mall user hash hash\n",
       "31935    ugh tri enjoy happi hour drink amp talk polit ...\n",
       "31936    want know live life thing make happi less thin...\n",
       "31937                                          love island\n",
       "31938    fav actor hash fav actress user fav director u...\n",
       "31939                                    whew product hash\n",
       "31940                                      user final user\n",
       "31941    pass first year uni hash hash hash hash hash h...\n",
       "31942                         week fli hash hash hash hash\n",
       "31943    user model photoshoot friday yay hash hash has...\n",
       "31944             surround peopl love even deserv yet hate\n",
       "31945         feel like hash hash hash hash hash hash hash\n",
       "31946             user omfg offend mailbox proud hash hash\n",
       "31947    user user ball hashtag say weasel away lumpi t...\n",
       "31948                    make ask anybodi god oh thank god\n",
       "31949    hear one new song go kati elli hash hash hash ...\n",
       "31950       user tri tail us stop butt good time hash hash\n",
       "31951                         post new blog hash hash hash\n",
       "31952                                   user went far user\n",
       "31953    good morn hash hash hash hash hash hash hash h...\n",
       "31954              hash bull domin bull direct whatev want\n",
       "31955                                less 2 week user hash\n",
       "31956      fish tomorrow user carnt wait first time 2 year\n",
       "31957                                   ate user isz youuu\n",
       "31958    see nina turner airwav tri wrap mantl genuin h...\n",
       "31959             listen sad song monday morn otw work sad\n",
       "31960        user hash hash vandalis hash hash condemn act\n",
       "31961                                    thank user follow\n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the words stem using Snowball Stemmer\n",
    "\n",
    "from nltk.stem import *\n",
    "\n",
    "SS = SnowballStemmer(\"english\")\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join(SS.stem(word) for word in x.split()))\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Creating a Bag of Words\n",
    "\n",
    "words = []\n",
    "\n",
    "for text in tweets:\n",
    "    word = word_tokenize(text)\n",
    "    for i in word:\n",
    "        words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words 16926\n",
      "First 30 most common words [('hash', 74900), ('user', 17534), ('day', 2554), ('amp', 1749), ('happi', 1740), ('love', 1557), ('get', 1252), ('time', 1211), ('u', 1170), ('go', 1144), ('thank', 1043), ('like', 1032), ('today', 1029), ('make', 966), ('new', 929), ('see', 864), ('one', 835), ('peopl', 832), ('good', 810), ('want', 778), ('father', 746), ('life', 734), ('take', 730), ('look', 710), ('feel', 694), ('need', 654), ('wait', 642), ('come', 636), ('work', 627), ('2', 613)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "words = nltk.FreqDist(words)\n",
    "\n",
    "print (\"Total Number of words {}\".format(len(words)))\n",
    "print (\"First 30 most common words {}\".format(words.most_common(30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the first 5000 words as Features\n",
    "\n",
    "word_features = list(words.keys())[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding if a word in the word_features is present in the tweets\n",
    "def finding_features(tweet):\n",
    "    text = word_tokenize(tweet)\n",
    "    features={}\n",
    "    for i in word_features:\n",
    "        features[i] = (i in text)\n",
    "    return features\n",
    "\n",
    "# Zipping the Processed tweets with the Labels\n",
    "tweets_featlab = zip(tweets, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the finding_feature function for all the tweets\n",
    "feature_set = [(finding_features(TW) ,label) for (TW,label) in tweets_featlab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 15981\n",
      "Testing Size: 15981\n"
     ]
    }
   ],
   "source": [
    "seed=1\n",
    "np.random.seed = seed\n",
    "\n",
    "# Splitting Training and Testing Datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(feature_set, test_size = 0.50, random_state = seed)\n",
    "\n",
    "print ('Training Size: {}'.format(len(train)))\n",
    "print ('Testing Size: {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision tree: 92.06557787372505\n"
     ]
    }
   ],
   "source": [
    "# Model_1 DecisionTreeClassifier\n",
    "\n",
    "nltk_model1 = SklearnClassifier(DecisionTreeClassifier())\n",
    "nltk_model1.train(train)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(nltk_model1, test)*100\n",
    "print (\"Accuracy of Decision tree: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SGD: 94.4934609849196\n"
     ]
    }
   ],
   "source": [
    "# Model_2 Stochastic Gradient Descent\n",
    "\n",
    "nltk_model2 = SklearnClassifier(SGDClassifier(max_iter = 1000))\n",
    "nltk_model2.train(train)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(nltk_model2, test)*100\n",
    "print (\"Accuracy of SGD: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest: 94.23064889556349\n"
     ]
    }
   ],
   "source": [
    "# Model_3 RandomForestClassifier\n",
    "\n",
    "nltk_model3 = SklearnClassifier(RandomForestClassifier())\n",
    "nltk_model3.train(train)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(nltk_model3, test)*100\n",
    "print (\"Accuracy of Random Forest: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 94.5622927226081\n"
     ]
    }
   ],
   "source": [
    "# Model_4 Logistic Regression\n",
    "\n",
    "nltk_model4 = SklearnClassifier(LogisticRegression())\n",
    "nltk_model4.train(train)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(nltk_model4, test)*100\n",
    "print (\"Accuracy of Logistic Regression: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     29720\n",
      "           1       0.71      0.70      0.71      2242\n",
      "\n",
      "    accuracy                           0.96     31962\n",
      "   macro avg       0.84      0.84      0.84     31962\n",
      "weighted avg       0.96      0.96      0.96     31962\n",
      "\n",
      "Report for SGD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     29720\n",
      "           1       0.85      0.44      0.58      2242\n",
      "\n",
      "    accuracy                           0.96     31962\n",
      "   macro avg       0.90      0.72      0.78     31962\n",
      "weighted avg       0.95      0.96      0.95     31962\n",
      "\n",
      "Report for Random forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     29720\n",
      "           1       0.86      0.64      0.73      2242\n",
      "\n",
      "    accuracy                           0.97     31962\n",
      "   macro avg       0.92      0.81      0.86     31962\n",
      "weighted avg       0.97      0.97      0.97     31962\n",
      "\n",
      "Report for Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.97     29720\n",
      "           1       0.85      0.38      0.53      2242\n",
      "\n",
      "    accuracy                           0.95     31962\n",
      "   macro avg       0.90      0.69      0.75     31962\n",
      "weighted avg       0.95      0.95      0.94     31962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_1, label = zip(*feature_set)\n",
    "\n",
    "# Classification Report and Confusion Matrix for the Models\n",
    "models = [nltk_model1, nltk_model2, nltk_model3, nltk_model4]\n",
    "classifiers = ['Decision Tree', 'SGD', 'Random forest', 'Logistic Regression']\n",
    "i = 0\n",
    "for model in models:\n",
    "    predictions = model.classify_many(test_1)\n",
    "    class_=classifiers[i]\n",
    "    print (\"Report for {}\".format(class_))\n",
    "    print (classification_report(label, predictions))\n",
    "    pd.DataFrame(confusion_matrix(label, predictions))\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
